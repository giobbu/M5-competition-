{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "from math import log, floor\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pywt\n",
    "from statsmodels.robust import mad\n",
    "\n",
    "import scipy\n",
    "import statsmodels\n",
    "from scipy import signal\n",
    "import statsmodels.api as sm\n",
    "from fbprophet import Prophet\n",
    "from scipy.signal import butter, deconvolve\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor, GradientBoostingRegressor\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, parallel_backend\n",
    "from joblib import delayed\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "SEED = 42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random hyperparammeters\n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'metric': 'rmse',\n",
    "#     'objective': 'regression',\n",
    "#     'n_jobs': -1,\n",
    "#     'seed': 236,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'bagging_fraction': 0.75,\n",
    "#     'bagging_freq': 10, \n",
    "#     'colsample_bytree': 0.75}\n",
    "\n",
    "# # define random hyperparammeters\n",
    "\n",
    "# params = { 'boosting_type': 'gbdt',\n",
    "#                     'objective': 'tweedie',\n",
    "#                     'tweedie_variance_power': 1.1,\n",
    "#                     'metric': 'rmse',\n",
    "#                     'subsample': 0.5,\n",
    "#                     'subsample_freq': 1,\n",
    "#                     'learning_rate': 0.05,\n",
    "#                     'num_leaves': 2**11-1,\n",
    "#                     'min_data_in_leaf': 2**12-1,\n",
    "#                     'feature_fraction': 0.5,\n",
    "#                     'max_bin': 100,\n",
    "#                     'n_estimators': 1400,\n",
    "#                     'boost_from_average': False,\n",
    "#                     'verbose': -1} \n",
    "\n",
    "      \n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'n_jobs': -1,\n",
    "#     'objective': 'poisson',\n",
    "#     'metric': 'rmse',\n",
    "#     'seed': 42,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'bagging_fraction': 0.85,\n",
    "#     'bagging_freq': 1,\n",
    "#     'colsample_bytree': 0.85,\n",
    "#     'colsample_bynode': 0.85,\n",
    "#     'min_data_per_leaf': 25,\n",
    "#     'n_estimators': 1400,\n",
    "#     'lambda_l1': 0.5,\n",
    "#     'lambda_l2': 0.5}\n",
    "\n",
    "\n",
    "\n",
    "# params = {\n",
    "#         \"objective\" : \"poisson\",\n",
    "#         \"metric\" :\"rmse\",\n",
    "#         \"force_row_wise\" : True,\n",
    "#         \"learning_rate\" : 0.075,\n",
    "# #         \"sub_feature\" : 0.8,\n",
    "#         \"sub_row\" : 0.75,\n",
    "#         \"bagging_freq\" : 1,\n",
    "#         \"lambda_l2\" : 0.1,\n",
    "# #         \"nthread\" : 4\n",
    "#         \"metric\": [\"rmse\"],\n",
    "#     'verbosity': 1,\n",
    "#     'num_iterations' : 1200,\n",
    "#     'num_leaves': 128,\n",
    "#     \"min_data_in_leaf\": 100,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': ['rmse'],           \n",
    "                    'subsample': 0.5,                \n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,           \n",
    "                    'num_leaves': 2**8-1,            \n",
    "                    'min_data_in_leaf': 2**8-1,     \n",
    "                    'feature_fraction': 0.8,\n",
    "                    'n_estimators': 1500,            \n",
    "                    'early_stopping_rounds': 30,     \n",
    "                    'seed': SEED,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "\n",
    "\n",
    "# params = {\n",
    "#         \"objective\" : \"poisson\",\n",
    "#         \"metric\" :\"rmse\",\n",
    "#         \"force_row_wise\" : True,\n",
    "#         \"learning_rate\" : 0.075,\n",
    "# #         \"sub_feature\" : 0.8,\n",
    "#         \"sub_row\" : 0.75,\n",
    "#         \"bagging_freq\" : 1,\n",
    "#         \"lambda_l2\" : 0.1,\n",
    "# #         \"nthread\" : 4\n",
    "#         \"metric\": [\"rmse\"],\n",
    "#     'verbosity': 1,\n",
    "#     'num_iterations' : 1200,\n",
    "#     'num_leaves': 128,\n",
    "#     'seed': SEED,\n",
    "#     \"min_data_in_leaf\": 100,\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"../input/1-stats-id/1_list_cal_CA.pkl\", \"rb\") as x1:\n",
    "#     X1 = x1.read()\n",
    "    \n",
    "# X_ca_1 = pickle.loads(X1)\n",
    "# pd.DataFrame(X_ca_1[0]).iloc[:-28,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../input/1-stats-id/1_X_CA.pkl\", \"rb\") as x1:\n",
    "    X1 = x1.read()\n",
    "    \n",
    "with open(\"../input/1-2-stats-id/2_X_CA.pkl\", \"rb\") as x1_1:\n",
    "    X1_1 = x1_1.read()\n",
    "    \n",
    "del x1, x1_1 \n",
    "gc.collect()\n",
    "\n",
    "X_ca_1 = pickle.loads(X1)\n",
    "X_ca_2 = pickle.loads(X1_1)\n",
    "\n",
    "X_ca = [X_ca_1, X_ca_2]\n",
    "\n",
    "del X1, X1_1, X_ca_1, X_ca_2 \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10573932, 65)\n",
      "\n",
      "number of STORES\n",
      "[0. 1.]\n",
      "\n",
      "\n",
      "store 0.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 2.24251\n",
      "[200]\tvalid_0's rmse: 2.18811\n",
      "[300]\tvalid_0's rmse: 2.14389\n",
      "[400]\tvalid_0's rmse: 2.10562\n",
      "[500]\tvalid_0's rmse: 2.07145\n",
      "[600]\tvalid_0's rmse: 2.04096\n",
      "[700]\tvalid_0's rmse: 2.01311\n",
      "[800]\tvalid_0's rmse: 1.98602\n",
      "[900]\tvalid_0's rmse: 1.95882\n",
      "[1000]\tvalid_0's rmse: 1.93476\n",
      "[1100]\tvalid_0's rmse: 1.9123\n",
      "[1200]\tvalid_0's rmse: 1.89185\n",
      "[1300]\tvalid_0's rmse: 1.87121\n",
      "[1400]\tvalid_0's rmse: 1.852\n",
      "[1500]\tvalid_0's rmse: 1.83419\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.83419\n",
      "save model_0_0\n",
      "\n",
      "\n",
      "store 1.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.7055\n",
      "[200]\tvalid_0's rmse: 1.66828\n",
      "[300]\tvalid_0's rmse: 1.63876\n",
      "[400]\tvalid_0's rmse: 1.61261\n",
      "[500]\tvalid_0's rmse: 1.58956\n",
      "[600]\tvalid_0's rmse: 1.56725\n",
      "[700]\tvalid_0's rmse: 1.54724\n",
      "[800]\tvalid_0's rmse: 1.52737\n",
      "[900]\tvalid_0's rmse: 1.50979\n",
      "[1000]\tvalid_0's rmse: 1.49324\n",
      "[1100]\tvalid_0's rmse: 1.47758\n",
      "[1200]\tvalid_0's rmse: 1.46236\n",
      "[1300]\tvalid_0's rmse: 1.44829\n",
      "[1400]\tvalid_0's rmse: 1.43452\n",
      "[1500]\tvalid_0's rmse: 1.42079\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.42079\n",
      "save model_1_1\n",
      "\n",
      "(10573932, 65)\n",
      "\n",
      "number of STORES\n",
      "[0. 1.]\n",
      "\n",
      "\n",
      "store 0.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 3.16567\n",
      "[200]\tvalid_0's rmse: 3.08313\n",
      "[300]\tvalid_0's rmse: 3.02274\n",
      "[400]\tvalid_0's rmse: 2.97078\n",
      "[500]\tvalid_0's rmse: 2.92296\n",
      "[600]\tvalid_0's rmse: 2.88018\n",
      "[700]\tvalid_0's rmse: 2.84253\n",
      "[800]\tvalid_0's rmse: 2.80482\n",
      "[900]\tvalid_0's rmse: 2.76844\n",
      "[1000]\tvalid_0's rmse: 2.73625\n",
      "[1100]\tvalid_0's rmse: 2.70466\n",
      "[1200]\tvalid_0's rmse: 2.67688\n",
      "[1300]\tvalid_0's rmse: 2.64953\n",
      "[1400]\tvalid_0's rmse: 2.62248\n",
      "[1500]\tvalid_0's rmse: 2.59699\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 2.59699\n",
      "save model_2_0\n",
      "\n",
      "\n",
      "store 1.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.31097\n",
      "[200]\tvalid_0's rmse: 1.28345\n",
      "[300]\tvalid_0's rmse: 1.26078\n",
      "[400]\tvalid_0's rmse: 1.24077\n",
      "[500]\tvalid_0's rmse: 1.22313\n",
      "[600]\tvalid_0's rmse: 1.20667\n",
      "[700]\tvalid_0's rmse: 1.19122\n",
      "[800]\tvalid_0's rmse: 1.17652\n",
      "[900]\tvalid_0's rmse: 1.16337\n",
      "[1000]\tvalid_0's rmse: 1.15003\n",
      "[1100]\tvalid_0's rmse: 1.13834\n",
      "[1200]\tvalid_0's rmse: 1.12692\n",
      "[1300]\tvalid_0's rmse: 1.11646\n",
      "[1400]\tvalid_0's rmse: 1.10616\n",
      "[1500]\tvalid_0's rmse: 1.09626\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.09626\n",
      "save model_3_1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "for xi in X_ca:\n",
    "    \n",
    "    print(xi.shape)\n",
    "    \n",
    "    STORES = np.unique(xi[:,3])\n",
    "    print('')\n",
    "    print('number of STORES')\n",
    "    print(STORES)\n",
    "    print('')\n",
    "\n",
    "    for store in STORES:\n",
    "\n",
    "        print('')\n",
    "        print('store '+str(store))\n",
    "        print('')\n",
    "        xi_store = xi[xi[:,3]==store]\n",
    "        \n",
    "\n",
    "        #18 -day\n",
    "        #19 -month\n",
    "        #20 - year\n",
    "\n",
    "        mask_XYtrain = np.logical_or(xi_store[:,21]<=24,xi_store[:,22]<=4,xi_store[:,23]<=2016)\n",
    "\n",
    "        # <= '2016-03-27' ~\n",
    "        XY_train =  xi_store#[mask_XYtrain]\n",
    "        XY_val =  xi_store[~mask_XYtrain]\n",
    "\n",
    "#         XY_train = np.delete(XY_train,[18,19,20],1)\n",
    "        XY_train = np.delete(XY_train,[3,4],1)\n",
    "        XY_val = np.delete(XY_val,[3,4],1)\n",
    "\n",
    "        X_train = XY_train[:,:-1]\n",
    "        y_train = XY_train[:,-1]\n",
    "        X_val = XY_val[:,:-1]\n",
    "        y_val = XY_val[:,-1]\n",
    "\n",
    "        del xi_store, XY_train, XY_val\n",
    "        gc.collect()\n",
    "\n",
    "        train_set = lgb.Dataset(X_train, y_train.reshape(-1))\n",
    "        val_set = lgb.Dataset(X_val, y_val.reshape(-1))\n",
    "\n",
    "        seed_everything(SEED)\n",
    "        model_ca = lgb.train(params, train_set,\n",
    "                           valid_sets = [val_set],\n",
    "#                           num_boost_round = 2500,\n",
    "#                           early_stopping_rounds = 60,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "    #         model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 500, \n",
    "    #                           valid_sets = [train_set, val_set], verbose_eval = 50)\n",
    "\n",
    "        model_ca.save_model('model_ca_'+str(i)+'.txt')\n",
    "        print('save model_'+str(i)+'_'+str(int(store)))\n",
    "        print('')\n",
    "\n",
    "        del train_set, val_set, model_ca\n",
    "        gc.collect()\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "    del xi\n",
    "    gc.collect()\n",
    "\n",
    "del X_ca\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../input/2-stats-id/1_X_TX.pkl\", \"rb\") as x1:\n",
    "    X1 = x1.read()\n",
    "\n",
    "with open(\"../input/2-2-stats-id/2_X_TX.pkl\", \"rb\") as x2:\n",
    "    X2 = x2.read()\n",
    "    \n",
    "X_tx_1 = pickle.loads(X1)\n",
    "X_tx_2 = pickle.loads(X2)\n",
    "\n",
    "del x1,x2, X1, X2\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "X_tx = [X_tx_1, X_tx_2]\n",
    "\n",
    "del X_tx_1, X_tx_2 \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10573932, 65)\n",
      "\n",
      "number of STORES\n",
      "[0. 1.]\n",
      "\n",
      "\n",
      "store 0.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.84278\n",
      "[200]\tvalid_0's rmse: 1.79778\n",
      "[300]\tvalid_0's rmse: 1.76137\n",
      "[400]\tvalid_0's rmse: 1.73303\n",
      "[500]\tvalid_0's rmse: 1.70461\n",
      "[600]\tvalid_0's rmse: 1.68046\n",
      "[700]\tvalid_0's rmse: 1.65649\n",
      "[800]\tvalid_0's rmse: 1.63223\n",
      "[900]\tvalid_0's rmse: 1.61158\n",
      "[1000]\tvalid_0's rmse: 1.59242\n",
      "[1100]\tvalid_0's rmse: 1.5714\n",
      "[1200]\tvalid_0's rmse: 1.55265\n",
      "[1300]\tvalid_0's rmse: 1.5355\n",
      "[1400]\tvalid_0's rmse: 1.51822\n",
      "[1500]\tvalid_0's rmse: 1.50261\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.50261\n",
      "save model_0\n",
      "\n",
      "\n",
      "store 1.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 2.1735\n",
      "[200]\tvalid_0's rmse: 2.12133\n",
      "[300]\tvalid_0's rmse: 2.07263\n",
      "[400]\tvalid_0's rmse: 2.03386\n",
      "[500]\tvalid_0's rmse: 1.99607\n",
      "[600]\tvalid_0's rmse: 1.96509\n",
      "[700]\tvalid_0's rmse: 1.93397\n",
      "[800]\tvalid_0's rmse: 1.9045\n",
      "[900]\tvalid_0's rmse: 1.87801\n",
      "[1000]\tvalid_0's rmse: 1.8517\n",
      "[1100]\tvalid_0's rmse: 1.82931\n",
      "[1200]\tvalid_0's rmse: 1.80762\n",
      "[1300]\tvalid_0's rmse: 1.78735\n",
      "[1400]\tvalid_0's rmse: 1.76575\n",
      "[1500]\tvalid_0's rmse: 1.74564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.74564\n",
      "save model_1\n",
      "\n",
      "(5286966, 65)\n",
      "\n",
      "number of STORES\n",
      "[0.]\n",
      "\n",
      "\n",
      "store 0.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.94844\n",
      "[200]\tvalid_0's rmse: 1.90604\n",
      "[300]\tvalid_0's rmse: 1.86823\n",
      "[400]\tvalid_0's rmse: 1.83558\n",
      "[500]\tvalid_0's rmse: 1.80501\n",
      "[600]\tvalid_0's rmse: 1.77915\n",
      "[700]\tvalid_0's rmse: 1.75276\n",
      "[800]\tvalid_0's rmse: 1.72913\n",
      "[900]\tvalid_0's rmse: 1.70607\n",
      "[1000]\tvalid_0's rmse: 1.68517\n",
      "[1100]\tvalid_0's rmse: 1.66535\n",
      "[1200]\tvalid_0's rmse: 1.64615\n",
      "[1300]\tvalid_0's rmse: 1.62827\n",
      "[1400]\tvalid_0's rmse: 1.61086\n",
      "[1500]\tvalid_0's rmse: 1.59439\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.59439\n",
      "save model_2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "for xi in X_tx:\n",
    "    \n",
    "    print(xi.shape)\n",
    "    \n",
    "    STORES = np.unique(xi[:,3])\n",
    "    print('')\n",
    "    print('number of STORES')\n",
    "    print(STORES)\n",
    "    print('')\n",
    "\n",
    "\n",
    "    for store in STORES:\n",
    "\n",
    "        print('')\n",
    "        print('store '+str(store))\n",
    "        print('')\n",
    "        xi_store = xi[xi[:,3]==store]\n",
    "\n",
    "        #18 -day\n",
    "        #19 -month\n",
    "        #20 - year\n",
    "\n",
    "        mask_XYtrain = np.logical_or(xi_store[:,21]<=24,xi_store[:,22]<=4,xi_store[:,23]<=2016)\n",
    "\n",
    "        # <= '2016-03-27' ~\n",
    "        XY_train =  xi_store#[mask_XYtrain]\n",
    "        XY_val =  xi_store[~mask_XYtrain]\n",
    "\n",
    "        XY_train = np.delete(XY_train,[3,4],1)\n",
    "        XY_val = np.delete(XY_val,[3,4],1)\n",
    "\n",
    "        X_train = XY_train[:,:-1]\n",
    "        y_train = XY_train[:,-1]\n",
    "        X_val = XY_val[:,:-1]\n",
    "        y_val = XY_val[:,-1]\n",
    "\n",
    "        del xi_store, XY_train, XY_val\n",
    "        gc.collect()\n",
    "\n",
    "        train_set = lgb.Dataset(X_train, y_train.reshape(-1))\n",
    "        val_set = lgb.Dataset(X_val, y_val.reshape(-1))\n",
    "\n",
    "        seed_everything(SEED)\n",
    "        model_tx = lgb.train(params, train_set,\n",
    "                           valid_sets = [val_set],\n",
    "#                           num_boost_round = 2500,\n",
    "#                           early_stopping_rounds = 60,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "        model_tx.save_model('model_tx_'+str(i)+'.txt')\n",
    "        print('save model_'+str(i))\n",
    "        print('')\n",
    "\n",
    "        del train_set, val_set, model_tx\n",
    "        gc.collect()\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "    del xi\n",
    "    gc.collect()\n",
    "    \n",
    "del X_tx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../input/3-stats-id/1_X_WI.pkl\", \"rb\") as x1:\n",
    "    X1 = x1.read()\n",
    "\n",
    "with open(\"../input/3-2-stats-id/2_X_WI.pkl\", \"rb\") as x2:\n",
    "    X2 = x2.read()\n",
    "    \n",
    "X_wi_1 = pickle.loads(X1)\n",
    "X_wi_2 = pickle.loads(X2)\n",
    "\n",
    "del x1,x2, X1, X2\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "X_wi = [X_wi_1, X_wi_2]\n",
    "\n",
    "del X_wi_1, X_wi_2 \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10573932, 65)\n",
      "\n",
      "number of STORES\n",
      "[0. 1.]\n",
      "\n",
      "\n",
      "store 0.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.49502\n",
      "[200]\tvalid_0's rmse: 1.46289\n",
      "[300]\tvalid_0's rmse: 1.43805\n",
      "[400]\tvalid_0's rmse: 1.41597\n",
      "[500]\tvalid_0's rmse: 1.39618\n",
      "[600]\tvalid_0's rmse: 1.37789\n",
      "[700]\tvalid_0's rmse: 1.36077\n",
      "[800]\tvalid_0's rmse: 1.34455\n",
      "[900]\tvalid_0's rmse: 1.32984\n",
      "[1000]\tvalid_0's rmse: 1.31588\n",
      "[1100]\tvalid_0's rmse: 1.30271\n",
      "[1200]\tvalid_0's rmse: 1.28993\n",
      "[1300]\tvalid_0's rmse: 1.27789\n",
      "[1400]\tvalid_0's rmse: 1.26626\n",
      "[1500]\tvalid_0's rmse: 1.25496\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.25496\n",
      "save model_0\n",
      "\n",
      "\n",
      "store 1.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.80286\n",
      "[200]\tvalid_0's rmse: 1.7587\n",
      "[300]\tvalid_0's rmse: 1.72499\n",
      "[400]\tvalid_0's rmse: 1.69416\n",
      "[500]\tvalid_0's rmse: 1.66458\n",
      "[600]\tvalid_0's rmse: 1.63867\n",
      "[700]\tvalid_0's rmse: 1.61522\n",
      "[800]\tvalid_0's rmse: 1.59348\n",
      "[900]\tvalid_0's rmse: 1.57193\n",
      "[1000]\tvalid_0's rmse: 1.55171\n",
      "[1100]\tvalid_0's rmse: 1.53275\n",
      "[1200]\tvalid_0's rmse: 1.51529\n",
      "[1300]\tvalid_0's rmse: 1.49873\n",
      "[1400]\tvalid_0's rmse: 1.48275\n",
      "[1500]\tvalid_0's rmse: 1.46728\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.46728\n",
      "save model_1\n",
      "\n",
      "(5286966, 65)\n",
      "\n",
      "number of STORES\n",
      "[0.]\n",
      "\n",
      "\n",
      "store 0.0\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.94239\n",
      "[200]\tvalid_0's rmse: 1.88963\n",
      "[300]\tvalid_0's rmse: 1.85119\n",
      "[400]\tvalid_0's rmse: 1.81784\n",
      "[500]\tvalid_0's rmse: 1.78631\n",
      "[600]\tvalid_0's rmse: 1.75799\n",
      "[700]\tvalid_0's rmse: 1.73332\n",
      "[800]\tvalid_0's rmse: 1.70853\n",
      "[900]\tvalid_0's rmse: 1.68519\n",
      "[1000]\tvalid_0's rmse: 1.66182\n",
      "[1100]\tvalid_0's rmse: 1.64055\n",
      "[1200]\tvalid_0's rmse: 1.62083\n",
      "[1300]\tvalid_0's rmse: 1.60197\n",
      "[1400]\tvalid_0's rmse: 1.58324\n",
      "[1500]\tvalid_0's rmse: 1.56525\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.56525\n",
      "save model_2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "for xi in X_wi:\n",
    "    \n",
    "    print(xi.shape)\n",
    "    \n",
    "    STORES = np.unique(xi[:,3])\n",
    "    print('')\n",
    "    print('number of STORES')\n",
    "    print(STORES)\n",
    "    print('')\n",
    "\n",
    "    for store in STORES:\n",
    "\n",
    "        print('')\n",
    "        print('store '+str(store))\n",
    "        print('')\n",
    "        xi_store = xi[xi[:,3]==store]\n",
    "\n",
    "        #18 -day\n",
    "        #19 -month\n",
    "        #20 - year\n",
    "\n",
    "        mask_XYtrain = np.logical_or(xi_store[:,21]<=24,xi_store[:,22]<=4,xi_store[:,23]<=2016)\n",
    "\n",
    "        # <= '2016-03-27' ~\n",
    "        XY_train =  xi_store#[mask_XYtrain]\n",
    "        XY_val =  xi_store[~mask_XYtrain]\n",
    "\n",
    "        XY_train = np.delete(XY_train,[3,4],1)\n",
    "        XY_val = np.delete(XY_val,[3,4],1)\n",
    "\n",
    "        X_train = XY_train[:,:-1]\n",
    "        y_train = XY_train[:,-1]\n",
    "        X_val = XY_val[:,:-1]\n",
    "        y_val = XY_val[:,-1]\n",
    "\n",
    "        del xi_store, XY_train, XY_val\n",
    "        gc.collect()\n",
    "\n",
    "        train_set = lgb.Dataset(X_train, y_train.reshape(-1))\n",
    "        val_set = lgb.Dataset(X_val, y_val.reshape(-1))\n",
    "\n",
    "        seed_everything(SEED)\n",
    "        model_wi = lgb.train(params, train_set,\n",
    "                           valid_sets = [val_set],\n",
    "#                           num_boost_round = 2500,\n",
    "#                           early_stopping_rounds = 60,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "        model_wi.save_model('model_wi_'+str(i)+'.txt')\n",
    "        print('save model_'+str(i))\n",
    "        print('')\n",
    "\n",
    "        del train_set, val_set, model_wi\n",
    "        gc.collect()\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "    del xi\n",
    "    gc.collect()\n",
    "    \n",
    "del X_wi\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"../input/1-stats-id/1_X_CA.pkl\", \"rb\") as x1:\n",
    "#     X1 = x1.read()\n",
    "    \n",
    "# with open(\"../input/1-2-stats-id/2_X_CA.pkl\", \"rb\") as x1_1:\n",
    "#     X1_1 = x1_1.read()\n",
    "    \n",
    "# del x1, x1_1 \n",
    "# gc.collect()\n",
    "\n",
    "# X_ca_1 = pickle.loads(X1)\n",
    "# X_ca_2 = pickle.loads(X1_1)\n",
    "\n",
    "# X_ca = [X_ca_1, X_ca_2]\n",
    "\n",
    "# del X1, X1_1, X_ca_1, X_ca_2 \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "\n",
    "# for xi in X_ca:\n",
    "    \n",
    "#     print(xi.shape)\n",
    "    \n",
    "#     STORES = np.unique(xi[:,3])\n",
    "#     print('')\n",
    "#     print('number of STORES')\n",
    "#     print(STORES)\n",
    "#     print('')\n",
    "\n",
    "#     for store in STORES:\n",
    "\n",
    "#         print('')\n",
    "#         print('store '+str(i))\n",
    "#         print('')\n",
    "#         xi_store = xi[xi[:,3]==store]\n",
    "        \n",
    "#         print(xi_store[:2,:4])\n",
    "\n",
    "#         #18 -day\n",
    "#         #19 -month\n",
    "#         #20 - year\n",
    "\n",
    "#         mask_XYtrain = np.logical_or(xi_store[:,18]<=27,xi_store[:,19]<=3,xi_store[:,20]<=2016)\n",
    "\n",
    "#         # <= '2016-03-27' ~\n",
    "#         XY_train =  xi_store#[mask_XYtrain]\n",
    "#         XY_val =  xi_store[~mask_XYtrain]\n",
    "\n",
    "#         X_train = XY_train[:,:-1]\n",
    "#         y_train = XY_train[:,-1]\n",
    "#         X_val = XY_val[:,:-1]\n",
    "#         y_val = XY_val[:,-1]\n",
    "\n",
    "#         del xi_store, XY_train, XY_val\n",
    "#         gc.collect()\n",
    "\n",
    "#         train_set = lgb.Dataset(X_train, y_train.reshape(-1))\n",
    "#         val_set = lgb.Dataset(X_val, y_val.reshape(-1))\n",
    "        \n",
    "#         if i==0:\n",
    "            \n",
    "#             print(i)\n",
    "#             seed_everything(SEED)\n",
    "#             model_ca = lgb.train(params, train_set,\n",
    "#                                valid_sets = [val_set],\n",
    "# #                               num_boost_round = 2000,\n",
    "# #                               early_stopping_rounds = 60,\n",
    "#                               verbose_eval = 100)\n",
    "            \n",
    "    \n",
    "#         else:\n",
    "            \n",
    "#             print(i)\n",
    "            \n",
    "#             seed_everything(SEED)\n",
    "#             model_ca = lgb.train(params, train_set,\n",
    "#                                valid_sets = [val_set],\n",
    "# #                               num_boost_round = 2000,\n",
    "# #                               early_stopping_rounds = 60,\n",
    "#                               init_model = model_ca,\n",
    "#                               verbose_eval = 100)\n",
    "\n",
    "#         del train_set, val_set\n",
    "#         gc.collect()\n",
    "\n",
    "#         i+=1\n",
    "        \n",
    "#     del xi\n",
    "#     gc.collect()\n",
    "\n",
    "# del X_ca\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../input/2-stats-id/1_X_TX.pkl\", \"rb\") as x1:\n",
    "#     X1 = x1.read()\n",
    "\n",
    "# with open(\"../input/2-2-stats-id/2_X_TX.pkl\", \"rb\") as x2:\n",
    "#     X2 = x2.read()\n",
    "    \n",
    "# X_tx_1 = pickle.loads(X1)\n",
    "# X_tx_2 = pickle.loads(X2)\n",
    "\n",
    "# del x1,x2, X1, X2\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# X_tx = [X_tx_1, X_tx_2]\n",
    "\n",
    "# del X_tx_1, X_tx_2 \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "\n",
    "# for xi in X_tx:\n",
    "    \n",
    "#     print(xi.shape)\n",
    "    \n",
    "#     STORES = np.unique(xi[:,3])\n",
    "#     print('')\n",
    "#     print('number of STORES')\n",
    "#     print(STORES)\n",
    "#     print('')\n",
    "\n",
    "\n",
    "#     for store in STORES:\n",
    "\n",
    "#         print('')\n",
    "#         print('store '+str(i))\n",
    "#         print('')\n",
    "#         xi_store = xi[xi[:,3]==store]\n",
    "\n",
    "#         #18 -day\n",
    "#         #19 -month\n",
    "#         #20 - year\n",
    "\n",
    "#         mask_XYtrain = np.logical_or(xi_store[:,18]<=27,xi_store[:,19]<=3,xi_store[:,20]<=2016)\n",
    "\n",
    "#         # <= '2016-03-27' ~\n",
    "#         XY_train =  xi_store#[mask_XYtrain]\n",
    "#         XY_val =  xi_store[~mask_XYtrain]\n",
    "\n",
    "#         X_train = XY_train[:,:-1]\n",
    "#         y_train = XY_train[:,-1]\n",
    "#         X_val = XY_val[:,:-1]\n",
    "#         y_val = XY_val[:,-1]\n",
    "\n",
    "#         del xi_store, XY_train, XY_val\n",
    "#         gc.collect()\n",
    "\n",
    "#         train_set = lgb.Dataset(X_train, y_train.reshape(-1))\n",
    "#         val_set = lgb.Dataset(X_val, y_val.reshape(-1))\n",
    "\n",
    "#         if i==0:\n",
    "            \n",
    "#             seed_everything(SEED)\n",
    "#             model_tx = lgb.train(params, train_set,\n",
    "#                                valid_sets = [val_set],\n",
    "# #                               num_boost_round = 2000,\n",
    "# #                               early_stopping_rounds = 60,\n",
    "#                             init_model = model_ca,\n",
    "#                               verbose_eval = 100)\n",
    "#             del model_ca\n",
    "#             gc.collect()\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             seed_everything(SEED)\n",
    "#             model_tx = lgb.train(params, train_set,\n",
    "#                                valid_sets = [val_set],\n",
    "# #                               num_boost_round = 2000,\n",
    "# #                               early_stopping_rounds = 60,\n",
    "#                               init_model = model_tx,\n",
    "#                               verbose_eval = 100)\n",
    "        \n",
    "\n",
    "#         del train_set, val_set\n",
    "#         gc.collect()\n",
    "\n",
    "#         i+=1\n",
    "        \n",
    "#     del xi\n",
    "#     gc.collect()\n",
    "    \n",
    "# del X_tx\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../input/3-stats-id/1_X_WI.pkl\", \"rb\") as x1:\n",
    "#     X1 = x1.read()\n",
    "\n",
    "# with open(\"../input/3-2-stats-id/2_X_WI.pkl\", \"rb\") as x2:\n",
    "#     X2 = x2.read()\n",
    "    \n",
    "# X_wi_1 = pickle.loads(X1)\n",
    "# X_wi_2 = pickle.loads(X2)\n",
    "\n",
    "# del x1,x2, X1, X2\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# X_wi = [X_wi_1, X_wi_2]\n",
    "\n",
    "# del X_wi_1, X_wi_2 \n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "\n",
    "# for xi in X_wi:\n",
    "    \n",
    "#     print(xi.shape)\n",
    "    \n",
    "#     STORES = np.unique(xi[:,3])\n",
    "#     print('')\n",
    "#     print('number of STORES')\n",
    "#     print(STORES)\n",
    "#     print('')\n",
    "\n",
    "#     for store in STORES:\n",
    "\n",
    "#         print('')\n",
    "#         print('store '+str(i))\n",
    "#         print('')\n",
    "#         xi_store = xi[xi[:,3]==store]\n",
    "\n",
    "#         #18 -day\n",
    "#         #19 -month\n",
    "#         #20 - year\n",
    "\n",
    "#         mask_XYtrain = np.logical_or(xi_store[:,18]<=27,xi_store[:,19]<=3,xi_store[:,20]<=2016)\n",
    "\n",
    "#         # <= '2016-03-27' ~\n",
    "#         XY_train =  xi_store#[mask_XYtrain]\n",
    "#         XY_val =  xi_store[~mask_XYtrain]\n",
    "\n",
    "#         X_train = XY_train[:,:-1]\n",
    "#         y_train = XY_train[:,-1]\n",
    "#         X_val = XY_val[:,:-1]\n",
    "#         y_val = XY_val[:,-1]\n",
    "\n",
    "#         del xi_store, XY_train, XY_val\n",
    "#         gc.collect()\n",
    "\n",
    "#         train_set = lgb.Dataset(X_train, y_train.reshape(-1))\n",
    "#         val_set = lgb.Dataset(X_val, y_val.reshape(-1))\n",
    "        \n",
    "#         if i==0:\n",
    "\n",
    "#             seed_everything(SEED)\n",
    "#             model_wi = lgb.train(params, train_set,\n",
    "#                                valid_sets = [val_set],\n",
    "#                               init_model = model_tx,\n",
    "# #                               num_boost_round = 2000,  \n",
    "# #                               early_stopping_rounds = 60,\n",
    "#                               verbose_eval = 100)\n",
    "#             del model_tx\n",
    "#             gc.collect()\n",
    "            \n",
    "#         else:\n",
    "            \n",
    "#             seed_everything(SEED)\n",
    "#             model_wi = lgb.train(params, train_set,\n",
    "#                                valid_sets = [val_set],\n",
    "#                               init_model = model_wi,\n",
    "# #                               num_boost_round = 2000,   \n",
    "# #                               early_stopping_rounds = 60,\n",
    "#                               verbose_eval = 100)\n",
    "            \n",
    "#         if i ==2:\n",
    "            \n",
    "#             model_wi.save_model('model.txt')\n",
    "#             print('save model ')\n",
    "#             print('')\n",
    "\n",
    "#         del train_set, val_set\n",
    "#         gc.collect()\n",
    "\n",
    "#         i+=1\n",
    "        \n",
    "#     del xi\n",
    "#     gc.collect()\n",
    "    \n",
    "# del X_wi\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 7))\n",
    "# lgb.plot_importance(model, max_num_features=10, ax=ax)\n",
    "# plt.title(\"LightGBM - Feature Importance\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
